{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f6230b-ada8-45f3-8c4c-4932c9ef13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d670aae2-ff86-4a9f-ab6e-ec4e9b77c06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:18<00:00, 9388501.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Importiamo le librerie necessarie: transforms da torchvision per le trasformazioni delle immagini\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "# Definiamo le trasformazioni da applicare al dataset (normalizzare le immagini e convertirle in tensori)\n",
    "# transforms.Compose permette di concatenare più trasformazioni\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # Convertiamo le immagini in tensori (vettori multidimensionali) con valori normalizzati tra 0 e 1\n",
    "        transforms.ToTensor(), \n",
    "        \n",
    "        # Normalizziamo le immagini per avere valori medi centrati a 0.5 con deviazione standard 0.5\n",
    "        # Ogni canale dell'immagine (rosso, verde, blu) viene normalizzato separatamente\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "    ])\n",
    "\n",
    "# Scarica e carica il dataset CIFAR-10 per il training, salvando i dati nella cartella './data'\n",
    "# 'train=True' indica che stiamo scaricando il dataset di addestramento\n",
    "# 'download=True' permette di scaricare i dati se non sono già presenti nella directory specificata\n",
    "# 'transform=transform' applica le trasformazioni definite sopra (conversione in tensori e normalizzazione)\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "# Utilizziamo DataLoader per caricare il dataset in mini-batch per il training\n",
    "# 'batch_size=100' significa che ogni batch sarà composto da 100 immagini\n",
    "# 'shuffle=True' mescola i dati ad ogni epoca, utile per evitare correlazioni durante l'addestramento\n",
    "# 'num_workers=2' indica che due thread verranno utilizzati per caricare i dati in parallelo, velocizzando il caricamento\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# Scarica e carica il dataset CIFAR-10 per il test, questa volta 'train=False' indica che è il dataset di test\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "# Anche per il dataset di test utilizziamo DataLoader per caricare i dati\n",
    "# 'batch_size=100' indica che i batch di test avranno 100 immagini ciascuno\n",
    "# 'shuffle=False' poiché nel test non è necessario mescolare i dati\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# Definiamo le classi presenti nel dataset CIFAR-10\n",
    "# CIFAR-10 è composto da 10 categorie di oggetti differenti (aerei, auto, uccelli, gatti, cervi, cani, rane, cavalli, navi e camion)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e44618a7-76f1-4610-850e-551700e35748",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Definiamo una rete neurale convoluzionale (CNN) che eredita da nn.Module\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Inizializziamo la classe base nn.Module\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Primo strato convoluzionale: prende in input 3 canali (immagine a colori RGB), \n",
    "        # genera 32 canali (filtri), kernel di dimensione 3x3 e padding=1 per mantenere la dimensione dell'immagine\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        \n",
    "        # Secondo strato convoluzionale: prende 32 canali in input e genera 64 canali, \n",
    "        # kernel di dimensione 3x3 e padding=1 per mantenere la dimensione\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "\n",
    "        # Strato di pooling (max pooling): riduce la dimensione dell'immagine di un fattore 2x2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Primo strato completamente connesso (fully connected): input = 64 canali di feature map di dimensione 8x8,\n",
    "        # output = 512 neuroni. Viene utilizzato dopo la convoluzione e il pooling\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "\n",
    "        # Secondo strato completamente connesso (fully connected): input = 512 neuroni, output = 10 neuroni \n",
    "        # (corrispondenti alle 10 classi del CIFAR-10)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Applicazione della prima convoluzione, seguita da funzione di attivazione ReLU e pooling\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        \n",
    "        # Applicazione della seconda convoluzione, seguita da funzione di attivazione ReLU e pooling\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "\n",
    "        # \"Appiattimento\" del tensore: converte la feature map 2D in un vettore 1D, necessario per i layer fully connected.\n",
    "        # view(-1, 64 * 8 * 8) ridimensiona il tensore in modo che il primo asse (batch size) rimanga invariato,\n",
    "        # mentre tutte le altre dimensioni vengono compattate in un unico vettore.\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "\n",
    "        # Applicazione del primo strato fully connected con funzione di attivazione ReLU\n",
    "        x = torch.relu(self.fc1(x))\n",
    "\n",
    "        # Applicazione del secondo strato fully connected, che genera le previsioni finali (logits) per le 10 classi\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Restituisce l'output finale (senza softmax poiché il calcolo della loss la richiede separatamente)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34fe9151-7462-447e-b33e-7fd7e62c93bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo un'istanza del modello CNN precedentemente definito\n",
    "# 'model' è ora la nostra rete convoluzionale che utilizzeremo per l'addestramento e la predizione\n",
    "model = CNN()\n",
    "\n",
    "# Definiamo la funzione di perdita (loss function) per il compito di classificazione.\n",
    "# Utilizziamo 'CrossEntropyLoss', che è comunemente usata nei problemi di classificazione multi-classe.\n",
    "# Questa funzione calcola la differenza tra le previsioni del modello (logits) e le etichette reali (classi corrette).\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Definiamo l'ottimizzatore che sarà usato per aggiornare i pesi della rete durante l'addestramento.\n",
    "# Qui stiamo utilizzando 'Adam', un ottimizzatore avanzato che combina i vantaggi di Adagrad e RMSprop.\n",
    "# Adam adatta dinamicamente il tasso di apprendimento per ogni parametro, rendendolo molto efficace e robusto.\n",
    "# 'model.parameters()' passa i parametri del modello (pesi e bias) che devono essere ottimizzati.\n",
    "# 'lr=0.001' imposta il tasso di apprendimento iniziale a 0.001, che controlla la dimensione del passo\n",
    "# che l'ottimizzatore farà ad ogni aggiornamento dei pesi.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59039bd3-32f5-491e-9a49-351fa7620ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100: Loss 1.768\n",
      "Epoch 1, Batch 200: Loss 1.396\n",
      "Epoch 1, Batch 300: Loss 1.248\n",
      "Epoch 1, Batch 400: Loss 1.142\n",
      "Epoch 1, Batch 500: Loss 1.098\n",
      "Epoch 2, Batch 100: Loss 0.975\n",
      "Epoch 2, Batch 200: Loss 0.951\n",
      "Epoch 2, Batch 300: Loss 0.920\n",
      "Epoch 2, Batch 400: Loss 0.893\n",
      "Epoch 2, Batch 500: Loss 0.866\n",
      "Epoch 3, Batch 100: Loss 0.761\n",
      "Epoch 3, Batch 200: Loss 0.755\n",
      "Epoch 3, Batch 300: Loss 0.745\n",
      "Epoch 3, Batch 400: Loss 0.749\n",
      "Epoch 3, Batch 500: Loss 0.737\n",
      "Epoch 4, Batch 100: Loss 0.598\n",
      "Epoch 4, Batch 200: Loss 0.623\n",
      "Epoch 4, Batch 300: Loss 0.615\n",
      "Epoch 4, Batch 400: Loss 0.597\n",
      "Epoch 4, Batch 500: Loss 0.608\n",
      "Epoch 5, Batch 100: Loss 0.442\n",
      "Epoch 5, Batch 200: Loss 0.452\n",
      "Epoch 5, Batch 300: Loss 0.472\n",
      "Epoch 5, Batch 400: Loss 0.485\n",
      "Epoch 5, Batch 500: Loss 0.501\n",
      "Epoch 6, Batch 100: Loss 0.320\n",
      "Epoch 6, Batch 200: Loss 0.338\n",
      "Epoch 6, Batch 300: Loss 0.333\n",
      "Epoch 6, Batch 400: Loss 0.364\n",
      "Epoch 6, Batch 500: Loss 0.364\n",
      "Epoch 7, Batch 100: Loss 0.208\n",
      "Epoch 7, Batch 200: Loss 0.207\n",
      "Epoch 7, Batch 300: Loss 0.218\n",
      "Epoch 7, Batch 400: Loss 0.246\n",
      "Epoch 7, Batch 500: Loss 0.248\n",
      "Epoch 8, Batch 100: Loss 0.124\n",
      "Epoch 8, Batch 200: Loss 0.134\n",
      "Epoch 8, Batch 300: Loss 0.150\n",
      "Epoch 8, Batch 400: Loss 0.152\n",
      "Epoch 8, Batch 500: Loss 0.162\n",
      "Epoch 9, Batch 100: Loss 0.069\n",
      "Epoch 9, Batch 200: Loss 0.074\n",
      "Epoch 9, Batch 300: Loss 0.093\n",
      "Epoch 9, Batch 400: Loss 0.102\n",
      "Epoch 9, Batch 500: Loss 0.110\n",
      "Epoch 10, Batch 100: Loss 0.075\n",
      "Epoch 10, Batch 200: Loss 0.066\n",
      "Epoch 10, Batch 300: Loss 0.067\n",
      "Epoch 10, Batch 400: Loss 0.081\n",
      "Epoch 10, Batch 500: Loss 0.095\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Addestramento del modello CNN\n",
    "# 'epoch' indica una passata completa sul dataset di addestramento\n",
    "# In questo ciclo, eseguiamo l'addestramento per 10 epoche (iterazioni sull'intero dataset)\n",
    "for epoch in range(10):  # ciclo sul dataset più volte\n",
    "    running_loss = 0.0  # Variabile per tracciare la perdita media durante l'epoca\n",
    "\n",
    "    # Enumeriamo i dati presenti nel 'trainloader', che contiene i batch di immagini e le rispettive etichette\n",
    "    # 'i' è l'indice del batch, 'data' è una lista che contiene gli input (immagini) e le etichette (classi corrette)\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Otteniamo gli input e le etichette dal batch corrente. 'data' è una lista [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Azzeriamo i gradienti dei parametri del modello (pesi e bias)\n",
    "        # Questo è necessario perché PyTorch accumula i gradienti ad ogni iterazione,\n",
    "        # quindi dobbiamo resettarli prima di ogni passo di ottimizzazione.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # **Forward pass**: Passiamo gli input attraverso il modello per ottenere le previsioni (output)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calcoliamo la perdita (loss) confrontando le previsioni del modello (outputs) con le etichette reali (labels)\n",
    "        # La funzione di perdita 'criterion' (CrossEntropyLoss) misura quanto le previsioni sono lontane dalle etichette corrette.\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # **Backward pass**: Calcoliamo i gradienti della perdita rispetto ai parametri del modello\n",
    "        # Questo passaggio calcola il gradiente per ciascun parametro in base all'errore commesso.\n",
    "        loss.backward()\n",
    "\n",
    "        # **Ottimizzazione**: Aggiorniamo i pesi del modello utilizzando l'ottimizzatore Adam,\n",
    "        # che utilizza i gradienti calcolati nel backward pass per modificare i parametri e ridurre la perdita.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Tracciamo la perdita accumulata per ogni batch per poter monitorare l'addestramento\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Ogni 100 batch, stampiamo le statistiche (ad esempio, la perdita media per gli ultimi 100 batch)\n",
    "        if i % 100 == 99:    # Stampiamo ogni 100 batch\n",
    "            # La perdita media per ogni 100 batch viene stampata su schermo per monitorare il progresso\n",
    "            print(f'Epoch {epoch + 1}, Batch {i + 1}: Loss {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0  # Reset della variabile running_loss per il prossimo gruppo di batch\n",
    "\n",
    "# Dopo aver completato tutte le epoche, l'addestramento è finito\n",
    "print('Training completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29dec947-ff8f-4b13-8b86-d5c40a802e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10,000 test images: 72.57%\n"
     ]
    }
   ],
   "source": [
    "# Inizializziamo le variabili per tenere traccia del numero totale di predizioni corrette e del numero totale di immagini\n",
    "correct = 0  # Numero di predizioni corrette\n",
    "total = 0    # Numero totale di immagini testate\n",
    "\n",
    "# Disabilitiamo il calcolo del gradiente durante il testing.\n",
    "# Questo è importante perché durante il testing non aggiorniamo i pesi, quindi non è necessario calcolare i gradienti,\n",
    "# risparmiando memoria e velocizzando il processo.\n",
    "with torch.no_grad():\n",
    "    # Iteriamo attraverso i batch del test loader (testloader), che contiene i dati di test\n",
    "    for data in testloader:\n",
    "        # Estraiamo le immagini e le etichette reali (labels) dal batch corrente\n",
    "        images, labels = data\n",
    "\n",
    "        # Passiamo le immagini attraverso il modello per ottenere le previsioni\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Otteniamo le predizioni dal modello.\n",
    "        # torch.max(outputs, 1) restituisce il valore massimo e l'indice della classe con il valore più alto\n",
    "        # lungo la dimensione 1 (le classi). Non ci interessa il valore massimo in sé, ma solo l'indice (classe predetta).\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Aggiorniamo il conteggio del totale delle immagini testate\n",
    "        total += labels.size(0)  # labels.size(0) restituisce il numero di immagini nel batch (batch size)\n",
    "\n",
    "        # Sommiamo il numero di predizioni corrette.\n",
    "        # (predicted == labels) restituisce un tensore booleano con True per ogni predizione corretta,\n",
    "        # sommiamo quindi tutti i True (che valgono 1) usando .sum().item() per ottenere il numero totale di corrette.\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calcoliamo e stampiamo l'accuratezza del modello sulle immagini di test.\n",
    "# L'accuratezza è la percentuale di predizioni corrette rispetto al totale delle immagini testate.\n",
    "# Viene moltiplicata per 100 per ottenere la percentuale.\n",
    "print(f'Accuracy of the network on the 10,000 test images: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cf1c16d-d9bd-4757-8060-900de8e52ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'cnn_cifar10.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2213b45-a265-4083-bfc3-afc58305a1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: plane\n",
      "{'plane': 64.62109684944153, 'car': 0.0011086426638939884, 'bird': 34.6878707408905, 'cat': 4.2893901763818576e-07, 'deer': 0.002944551124528516, 'dog': 0.023453608446288854, 'frog': 0.66287643276155, 'horse': 0.0006325258709694026, 'ship': 1.1265018429185147e-05, 'truck': 5.169644623492786e-06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbagn\\AppData\\Local\\Temp\\ipykernel_40052\\3224306955.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('cnn_cifar10.pth'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the CNN architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# CIFAR-10 class names\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Load the saved model\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load('cnn_cifar10.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Preprocess the custom image\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    img = Image.open(image_path)\n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Make predictions\n",
    "def predict_image(model, image_path):\n",
    "    img = preprocess_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    predicted_class = classes[predicted[0]]\n",
    "    print(f'Predicted Class: {predicted_class}')\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage: Upload and predict a custom image\n",
    "image_path = 'images.jpeg'\n",
    "predict_image(model, image_path)\n",
    "\n",
    "def predict_confidence(model, image_path):\n",
    "    img = preprocess_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        return {classes[i]: float(probabilities[0][i]) * 100 for i in range(len(classes))}\n",
    "\n",
    "print(predict_confidence(model, image_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff054f5-6a1f-4bf7-b313-e6466b65dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fastapi uvicorn pillow torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faf8de31-8469-4c57-a905-b4bee08129de",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importiamo le librerie necessarie da FastAPI per la gestione del server web e delle risposte\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastAPI, File, UploadFile\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JSONResponse\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Importiamo PyTorch per gestire il modello, la rete neurale e il preprocessing\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fastapi'"
     ]
    }
   ],
   "source": [
    "# Importiamo le librerie necessarie da FastAPI per la gestione del server web e delle risposte\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "from fastapi.responses import JSONResponse\n",
    "\n",
    "# Importiamo PyTorch per gestire il modello, la rete neurale e il preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Importiamo PIL per la gestione delle immagini\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Definiamo l'architettura della CNN (rete neurale convoluzionale)\n",
    "# Questa è una semplice rete che utilizziamo per fare previsioni su immagini del dataset CIFAR-10\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Primo strato convoluzionale: 3 canali in input (immagini RGB) e 32 canali in output (filtri)\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        # Secondo strato convoluzionale: 32 canali in input e 64 in output\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        # Strato di pooling per ridurre la dimensione (2x2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Primo strato completamente connesso (fully connected), dimensione 64*8*8 = 512 neuroni\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        # Secondo strato completamente connesso che riduce a 10 classi (per CIFAR-10)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    # Definizione del forward pass: specifica come i dati passano attraverso la rete\n",
    "    def forward(self, x):\n",
    "        # Primo strato convoluzionale, seguito da ReLU e Max Pooling\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        # Secondo strato convoluzionale, seguito da ReLU e Max Pooling\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        # Appiattiamo l'output convoluzionale in un vettore 1D per passare ai fully connected layer\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        # Primo fully connected layer seguito da ReLU\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # Secondo fully connected layer che restituisce l'output finale (logits per 10 classi)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Definiamo i nomi delle classi del dataset CIFAR-10\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Inizializziamo l'applicazione FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# Carichiamo il modello pre-addestrato salvato su file (cnn_cifar10.pth)\n",
    "# 'model.load_state_dict' carica i pesi nel modello definito sopra\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load('cnn_cifar10.pth'))\n",
    "model.eval()  # Impostiamo il modello in modalità \"evaluation\" per disabilitare il calcolo dei gradienti\n",
    "\n",
    "# Funzione per il preprocessing dell'immagine prima di passarla al modello\n",
    "# Questa funzione applica le trasformazioni necessarie all'immagine caricata dall'utente\n",
    "def preprocess_image(image_data):\n",
    "    # Definiamo una sequenza di trasformazioni per ridimensionare l'immagine, convertirla in tensore e normalizzarla\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),  # Ridimensioniamo l'immagine a 32x32 pixel (dimensione richiesta per CIFAR-10)\n",
    "        transforms.ToTensor(),  # Convertiamo l'immagine in un tensore\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalizziamo i canali R, G, B tra [-1, 1]\n",
    "    ])\n",
    "    \n",
    "    # Apriamo l'immagine da un flusso di byte in memoria (usiamo io.BytesIO per gestire i dati binari)\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "    \n",
    "    # Applichiamo le trasformazioni definite e aggiungiamo una dimensione batch (usiamo unsqueeze(0))\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    return image  # Restituiamo l'immagine preprocessata come tensore\n",
    "\n",
    "# Definiamo la rotta POST /predict per caricare un'immagine e fare la predizione\n",
    "# FastAPI gestisce il caricamento dell'immagine tramite il parametro 'file' usando 'UploadFile'\n",
    "@app.post(\"/predict/\")\n",
    "async def predict_image(file: UploadFile = File(...)):\n",
    "    # Leggiamo il contenuto del file caricato dall'utente\n",
    "    image_data = await file.read()\n",
    "\n",
    "    # Preprocessiamo l'immagine (ridimensionamento, conversione a tensore, normalizzazione)\n",
    "    img_tensor = preprocess_image(image_data)\n",
    "\n",
    "    # Disabilitiamo il calcolo dei gradienti (torch.no_grad) poiché siamo in modalità di inferenza (predizione)\n",
    "    with torch.no_grad():\n",
    "        # Passiamo l'immagine preprocessata attraverso il modello per ottenere i logits (output grezzo)\n",
    "        outputs = model(img_tensor)\n",
    "        \n",
    "        # torch.max ottiene l'indice della classe con il punteggio più alto\n",
    "        # Questo indice rappresenta la classe predetta dal modello\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Convertiamo l'indice della classe predetta nel nome della classe (ad esempio, 'dog', 'car', etc.)\n",
    "    predicted_class = classes[predicted[0]]\n",
    "    \n",
    "    # Restituiamo la classe predetta come risposta JSON al client\n",
    "    return JSONResponse(content={\"predicted_class\": predicted_class})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39655287-569f-49f0-8d29-66eef9b32702",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m uvicorn app:app --reload\n",
    "http://127.0.0.1:8000/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca25019-a704-4ff2-8aa1-da68cbe31c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.kaggle.com/datasets/zalando-research/fashionmnist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
